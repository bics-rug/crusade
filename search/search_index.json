{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CRUSADE","text":"<p>Conversion of Raw-audio Using Spikes Analog Digital Encoders</p> <p>This repository contains implementations of different analogue-to-spike converters for raw audio signal.</p>"},{"location":"#features","title":"Features","text":"<p>This project contains Python implementations of various methods and neural models used to transform audio signals into spike/event trains, using JAX. It includes: - Standalone ADM. - Filterbank with ADM. - Filterbank with Resonate and Fire neurons. - Filterbank with phase encoding (under development).</p>"},{"location":"#installation","title":"Installation","text":"<p>Recommended version for installation is using uv <pre><code>uv sync --frozen --all-extras\n</code></pre></p> <p>Or with pip</p> <pre><code>pip install \".[dev]\"\n</code></pre>"},{"location":"#example","title":"Example","text":"<p>Example for use <code>filterbank_ADM</code>:</p> <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom scipy.signal import chirp\n\nfrom crusafe.conversion_methods import filterbank_ADM\n\nsr = 44100\nduration = 0.1\nt = jnp.arange(int(sr * duration)) / sr\naudio = chirp(t, f0=200, f1=2000, t1=duration, method=\"linear\")\n\nfb = filterbank_ADM(\n    num_neurons=16, freq_min=200, freq_max=2000, freq_distribution=\"linear\"\n)\nevent_time, event_address, event_magnitude = fb(audio, sampling_rate=sr)\n\nplt.figure()\nplt.scatter(x=event_time, y=event_address, c=event_magnitude, cmap=\"bwr\")\nplt.show()\n</code></pre>"},{"location":"#tests","title":"Tests","text":"<ul> <li>On pc</li> </ul> <p>To check if the models are working: <pre><code>uv run pytest\n</code></pre> or if you are in the envirnoment</p> <pre><code>pytest\n</code></pre> <p>To check before commit:</p> <p><pre><code>uv run pre-commit run --all-files\n</code></pre> to install the  pre-commit (only forst time): <pre><code>uv run pre-commit install\n</code></pre> once installed it runs automatically for every commit</p> <ul> <li>On git</li> </ul> <p>It does automatically runs all the tests</p>"},{"location":"api/","title":"API Reference","text":"<p>API documentation for CRUSADE.</p>"},{"location":"api/#modules","title":"Modules","text":"<ul> <li>Conversion methods - Conversion methods</li> <li>Utils - Utility functions</li> </ul>"},{"location":"api/conversion_methods/","title":"Conversion methods","text":""},{"location":"api/conversion_methods/#crusade.conversion_methods","title":"<code>crusade.conversion_methods</code>","text":""},{"location":"api/conversion_methods/#crusade.conversion_methods-classes","title":"Classes","text":""},{"location":"api/conversion_methods/#crusade.conversion_methods.sigma_delta_neuron_in_the_loop","title":"<code>sigma_delta_neuron_in_the_loop</code>","text":"<p>Sigma-Delta IF Neuron Model Implements an adaptive sigma-delta neuron model that converts continuous audio signals into spike trains. The difference from the standard sigma-delta model is that the feed back spikes and not continuous values. IT has an integrator in series with a integrate and fire neuron.</p> <p>Attributes:</p> Name Type Description <code>sampling_rate</code> <p>The sampling rate of the input audio signal.</p> <code>feedback_gain</code> <code>float</code> <p>The feedback gain for the neuron model.</p> <code>threshold</code> <code>float</code> <p>The threshold for spike generation.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Converts the input audio signal into spike trains.</p> Source code in <code>crusade/conversion_methods.py</code> <pre><code>class sigma_delta_neuron_in_the_loop:\n    \"\"\"Sigma-Delta IF Neuron Model\n    Implements an adaptive sigma-delta neuron model that converts continuous audio signals into spike trains.\n    The difference from the standard sigma-delta model is that the feed back spikes and not continuous values.\n    IT has an integrator in series with a integrate and fire neuron.\n\n    Attributes:\n        sampling_rate: The sampling rate of the input audio signal.\n        feedback_gain (float): The feedback gain for the neuron model.\n        threshold (float): The threshold for spike generation.\n\n    Methods:\n        __call__(audio, feedback_gain=None, threshold=None): Converts the input audio signal into spike trains.\n    \"\"\"\n\n    def __init__(\n        self,\n        sampling_rate: float = 4410000,\n        feedback_gain: float = 2.47498e-07,\n        threshold: float = 3.22814e-13,\n    ):\n        self.sampling_rate = sampling_rate\n        self.feedback_gain = feedback_gain\n        self.threshold = threshold\n\n    def __call__(\n        self,\n        audio: Float[Array, \"#time\"],\n        feedback_gain: Optional[float] = None,\n        threshold: Optional[float] = None,\n        sampling_rate: Optional[float] = None,\n    ) -&gt; tuple[Float[Array, \"*time\"], Int8[Array, \"*time\"]]:\n        if feedback_gain is None:\n            feedback_gain = self.feedback_gain\n        if threshold is None:\n            threshold = self.threshold\n        if sampling_rate is None:\n            sampling_rate = self.sampling_rate\n\n        @jax.jit\n        def body_fun(state, jj):\n            ii, mem_previous, spike_previous = (\n                state  # Unpack integrator and memory state\n            )\n\n            ii = (\n                ii + jj / sampling_rate - spike_previous * feedback_gain\n            )  # First integrator stage (accumulate input)\n            mem = (\n                mem_previous + ii / sampling_rate\n            )  # Membrane potential update using neuron gain\n\n            # Generate spike if membrane potential crosses thresholds aeset membrane potential if spike occurred\n            spike = (mem &gt;= threshold).astype(int) - (mem &lt;= -threshold).astype(int)\n            mem = (\n                1 - jnp.abs(spike)\n            ) * mem  # Reset membrane potential if spike occurred\n\n            return (ii, mem, spike), spike\n\n        ii = 0\n        _, out_spikes = jax.lax.scan(body_fun, (ii, 0, 0), audio)\n        time_ax = jnp.linspace(0, len(audio) / sampling_rate, len(audio))\n        event_mask = out_spikes != 0\n        event_time = time_ax[event_mask]\n        event_address = (out_spikes[event_mask] == 1).astype(jnp.int8)\n        return event_time, event_address\n</code></pre>"},{"location":"api/conversion_methods/#crusade.conversion_methods.sigma_delta_spiking","title":"<code>sigma_delta_spiking</code>","text":"<p>Sigma-Delta Model with spiking comparator and feedback Implements a sigma-delta neuron model that converts continuous audio signals into spike trains.</p> <p>Attributes:</p> Name Type Description <code>sampling_rate</code> <p>The sampling rate of the input audio signal.</p> <code>feedback_gain</code> <code>float</code> <p>The feedback gain for the neuron model.</p> <code>threshold</code> <code>float</code> <p>The threshold for spike generation.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Converts the input audio signal into spike trains.</p> Source code in <code>crusade/conversion_methods.py</code> <pre><code>class sigma_delta_spiking:\n    \"\"\"Sigma-Delta Model with spiking comparator and feedback\n    Implements a sigma-delta neuron model that converts continuous audio signals into spike trains.\n\n    Attributes:\n        sampling_rate: The sampling rate of the input audio signal.\n        feedback_gain (float): The feedback gain for the neuron model.\n        threshold (float): The threshold for spike generation.\n\n    Methods:\n        __call__(audio, feedback_gain=None, threshold=None): Converts the input audio signal into spike trains.\n    \"\"\"\n\n    def __init__(\n        self,\n        sampling_rate: float = 4410000,\n        feedback_gain: float = 5.75174e-07,\n        threshold: float = 6.867776e-07,\n    ):\n        self.sampling_rate = sampling_rate\n        self.feedback_gain = feedback_gain\n        self.threshold = threshold\n\n    def __call__(\n        self,\n        audio: Float[Array, \"#time\"],\n        feedback_gain: Optional[float] = None,\n        threshold: Optional[float] = None,\n        sampling_rate: Optional[float] = None,\n    ) -&gt; tuple[Float[Array, \"*time\"], Int8[Array, \"*time\"]]:\n        if feedback_gain is None:\n            feedback_gain = self.feedback_gain\n        if threshold is None:\n            threshold = self.threshold\n        if sampling_rate is None:\n            sampling_rate = self.sampling_rate\n\n        @jax.jit\n        def body_fun(carry, input_val):\n            (integrator, mem_p, mem_n) = carry\n            quantized = jnp.sign(integrator)\n\n            mem_p = jnp.where(quantized == 1, mem_p + 1 / sampling_rate, mem_p)\n            mem_n = jnp.where(quantized == -1, mem_n + 1 / sampling_rate, mem_n)\n\n            spikes_p = jnp.where(mem_p &gt;= threshold, 1, 0)\n            spikes_n = jnp.where(mem_n &gt;= threshold, 1, 0)\n\n            mem_p = jnp.where(spikes_p == 1, 0, mem_p)\n            mem_n = jnp.where(spikes_n == 1, 0, mem_n)\n\n            spikes = spikes_p - spikes_n\n\n            new_integrator = (\n                integrator + input_val / sampling_rate - feedback_gain * spikes\n            )\n\n            return (new_integrator, mem_p, mem_n), spikes\n\n        initial_carry = (0.0, 0.0, 0.0)\n        _, out_spikes = jax.lax.scan(body_fun, initial_carry, audio)\n        time_ax = jnp.linspace(0, len(audio) / sampling_rate, len(audio))\n        event_mask = out_spikes != 0\n        event_time = time_ax[event_mask]\n        event_address = (out_spikes[event_mask] == 1).astype(jnp.int8)\n        return event_time, event_address\n</code></pre>"},{"location":"api/conversion_methods/#crusade.conversion_methods.sigma_delta_edge_spikes","title":"<code>sigma_delta_edge_spikes</code>","text":"<p>Sigma-Delta ADC which transmits only spikes generate during transitions.</p> <p>Attributes:</p> Name Type Description <code>sampling_rate</code> <p>The sampling rate of the input audio signal.</p> <code>feedback_gain</code> <code>float</code> <p>The feedback gain for the sigma delta, always 1.</p> <code>threshold</code> <code>float</code> <p>The threshold for spike generation.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Converts the input audio signal into spike trains.</p> Source code in <code>crusade/conversion_methods.py</code> <pre><code>class sigma_delta_edge_spikes:\n    \"\"\"Sigma-Delta ADC which transmits only spikes generate during transitions.\n\n    Attributes:\n        sampling_rate: The sampling rate of the input audio signal.\n        feedback_gain (float): The feedback gain for the sigma delta, always 1.\n        threshold (float): The threshold for spike generation.\n\n    Methods:\n        __call__(audio, feedback_gain=None, threshold=None): Converts the input audio signal into spike trains.\n    \"\"\"\n\n    def __init__(\n        self,\n        sampling_rate: float = 4410000,\n        feedback_gain: float = 1,\n        threshold: float = 1.98682e-06,\n    ):\n        self.sampling_rate = sampling_rate\n        self.feedback_gain = feedback_gain\n        self.threshold = threshold\n\n    def __call__(\n        self,\n        audio: Float[Array, \"#time\"],\n        feedback_gain: Optional[float] = None,\n        threshold: Optional[float] = None,\n        sampling_rate: Optional[float] = None,\n    ):\n        if feedback_gain is None:\n            feedback_gain = self.feedback_gain\n        if threshold is None:\n            threshold = self.threshold\n        if sampling_rate is None:\n            sampling_rate = self.sampling_rate\n\n        @jax.jit\n        def body_fun(carry, input_val):\n            (integrator, prev_quant) = carry\n            quantized = jnp.sign(integrator + threshold * prev_quant)\n            spk = jnp.where(quantized == prev_quant, 0, quantized)\n\n            new_integrator = (\n                integrator + (input_val - feedback_gain * quantized) / sampling_rate\n            )\n\n            return (new_integrator, quantized), spk\n\n        initial_carry = (0.0, 0.0)\n        _, out_spikes = jax.lax.scan(body_fun, initial_carry, audio)\n        time_ax = jnp.linspace(0, len(audio) / sampling_rate, len(audio))\n        event_mask = out_spikes != 0\n        event_time = time_ax[event_mask]\n        event_address = (out_spikes[event_mask] == 1).astype(jnp.int8)\n\n        return event_time, event_address\n</code></pre>"},{"location":"api/conversion_methods/#crusade.conversion_methods.amplitude_to_frequency","title":"<code>amplitude_to_frequency</code>","text":"<p>Amplitude to Frequency Conversion Converts the amplitude of the input audio signal into a frequency-modulated spike train.</p> <p>Attributes:</p> Name Type Description <code>sampling_rate</code> <p>The sampling rate of the input audio signal.</p> <code>max_freq</code> <code>float</code> <p>The maximum frequency of the output spike train.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Converts the input audio signal into a frequency-modulated spike train.</p> Source code in <code>crusade/conversion_methods.py</code> <pre><code>class amplitude_to_frequency:\n    \"\"\"Amplitude to Frequency Conversion\n    Converts the amplitude of the input audio signal into a frequency-modulated spike train.\n\n    Attributes:\n        sampling_rate: The sampling rate of the input audio signal.\n        max_freq (float): The maximum frequency of the output spike train.\n\n    Methods:\n        __call__(audio, sampling_rate=None, max_freq=None): Converts the input audio signal into a frequency-modulated spike train.\n    \"\"\"\n\n    def __init__(self, sampling_rate: float = 4410000, max_freq: float = 2000000):\n        self.sampling_rate = sampling_rate\n        self.max_freq = max_freq\n\n    def __call__(\n        self,\n        audio: Float[Array, \"#time\"],\n        sampling_rate: Optional[float] = None,\n        max_freq: Optional[float] = None,\n    ):\n        if sampling_rate is None:\n            sampling_rate = self.sampling_rate\n        if max_freq is None:\n            max_freq = self.max_freq\n\n        random_number = jax.random.uniform(jax.random.PRNGKey(0), shape=audio.shape)\n        spikes_p = random_number &lt; (audio * max_freq / sampling_rate)\n        spikes_n = random_number &lt; (-audio * max_freq / sampling_rate)\n        out_spikes = spikes_p.astype(int) - spikes_n.astype(int)\n\n        time_ax = jnp.linspace(0, len(audio) / sampling_rate, len(audio))\n        event_mask = out_spikes != 0\n\n        event_time = time_ax[event_mask]\n        event_address = (out_spikes[event_mask] == 1).astype(jnp.int8)\n\n        return event_time, event_address\n</code></pre>"},{"location":"api/conversion_methods/#crusade.conversion_methods.resonate_and_fire_bank","title":"<code>resonate_and_fire_bank</code>","text":"<p>Resonate-and-Fire Neuron Bank Implements a bank of resonate-and-fire neurons that convert continuous audio signals into spike trains.</p> <p>Attributes:</p> Name Type Description <code>sampling_rate</code> <p>The sampling rate of the input audio signal.</p> <code>num_neurons</code> <code>int</code> <p>The number of resonate-and-fire neurons in the bank.</p> <code>freq_min</code> <code>float</code> <p>The minimum frequency of the resonate-and-fire neurons.</p> <code>freq_max</code> <code>float</code> <p>The maximum frequency of the resonate-and-fire neurons.</p> <code>damping_factor</code> <code>float</code> <p>The damping factor for the resonate-and-fire neurons.</p> <code>input_gain</code> <code>float</code> <p>The gain applied to the input audio signal.</p> <code>freq_distribution</code> <code>str</code> <p>The distribution of frequencies for the resonate-and-fire neurons ('linear', 'log', 'mel').</p> <code>threshold</code> <code>float</code> <p>The threshold for spike generation.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Converts the input audio signal into spike trains.</p> Source code in <code>crusade/conversion_methods.py</code> <pre><code>class resonate_and_fire_bank:\n    \"\"\"Resonate-and-Fire Neuron Bank\n    Implements a bank of resonate-and-fire neurons that convert continuous audio signals into spike trains.\n\n    Attributes:\n        sampling_rate: The sampling rate of the input audio signal.\n        num_neurons (int): The number of resonate-and-fire neurons in the bank.\n        freq_min (float): The minimum frequency of the resonate-and-fire neurons.\n        freq_max (float): The maximum frequency of the resonate-and-fire neurons.\n        damping_factor (float): The damping factor for the resonate-and-fire neurons.\n        input_gain (float): The gain applied to the input audio signal.\n        freq_distribution (str): The distribution of frequencies for the resonate-and-fire neurons ('linear', 'log', 'mel').\n        threshold (float): The threshold for spike generation.\n\n    Methods:\n        __call__(audio, sampling_rate=None, damping_factor=None, input_gain=None, threshold=None): Converts the input audio signal into spike trains.\n    \"\"\"\n\n    # Adapted from Giuseppe Leo code\n\n    def __init__(\n        self,\n        sampling_rate: float = 4410000,\n        num_neurons: int = 32,\n        freq_min: float = 20,\n        freq_max: float = 20000,\n        damping_factor_scaling: float = 1,\n        input_gain: float = 10,\n        freq_distribution: str = \"linear\",\n        threshold: float = 1.0,\n        debug: bool = False,\n    ):\n        self.sampling_rate = sampling_rate\n        self.num_neurons = num_neurons\n        self.freq_min = freq_min\n        self.freq_max = freq_max\n        self.threshold = threshold\n        self.freq_distribution = freq_distribution\n        self.damping_factor_scaling = damping_factor_scaling\n        self.input_gain = input_gain\n        self.debug = debug\n\n        # Create frequency array for the resonate-and-fire neurons\n        self.frequencies, self.frequencies_bins = utils.frequency_bins_generator(\n            number_of_bins=self.num_neurons,\n            freq_min=self.freq_min,\n            freq_max=self.freq_max,\n            freq_distribution=self.freq_distribution,\n            bins_superimpose=0.0,\n        )\n\n        delta_freq = jnp.array(\n            [\n                self.frequencies_bins[i + 1] - self.frequencies_bins[i]\n                for i in range(len(self.frequencies_bins) - 1)\n            ]\n        )\n        self.omega = 2 * jnp.pi * self.frequencies\n        natural_omega = (\n            self.omega / jnp.sqrt(1 - ((delta_freq / self.frequencies) ** 2) / 2)\n        )  # here sometimes gives a NAN for the first neuron cause frequencies[1] sould be less than 3*frequencies[0]\n        damping_ratio = (2 * jnp.pi * delta_freq) / (2 * self.omega)\n        self.damping_factors = (\n            -damping_ratio * natural_omega * self.damping_factor_scaling\n        )\n\n        # factors for analitical solution\n        self.exp_damping = jnp.exp(self.damping_factors / sampling_rate)\n        self.cos_omega = jnp.cos(self.omega / sampling_rate)\n        self.sin_omega = jnp.sin(self.omega / sampling_rate)\n\n    def __call__(\n        self,\n        audio: Float[Array, \"#time\"],\n        sampling_rate: Optional[float] = None,\n        input_gain: Optional[float] = None,\n        threshold: Optional[float] = None,\n        damping_factor_scaling: Optional[float] = None,\n    ):\n        if sampling_rate is None:\n            sampling_rate = self.sampling_rate\n\n        if damping_factor_scaling is None:\n            damping_factor_scaling = self.damping_factor_scaling\n\n        if (sampling_rate != self.sampling_rate) or (\n            damping_factor_scaling != self.damping_factor_scaling\n        ):\n            self.sampling_rate = sampling_rate\n            self.damping_factor_scaling = damping_factor_scaling\n            self.exp_damping = jnp.exp(\n                self.damping_factors * damping_factor_scaling / sampling_rate\n            )\n            self.cos_omega = jnp.cos(self.omega / sampling_rate)\n            self.sin_omega = jnp.sin(self.omega / sampling_rate)\n\n        if input_gain is None:\n            input_gain = self.input_gain\n        if threshold is None:\n            threshold = self.threshold\n\n        if self.debug:\n\n            @jax.jit\n            def body_fun(carry, input_val):\n                (x_prev, y_prev) = carry\n\n                # exact dynamics\n                x = (\n                    self.exp_damping\n                    * (x_prev * self.cos_omega - y_prev * self.sin_omega)\n                    + input_gain\n                    * input_val\n                    * self.damping_factors\n                    * damping_factor_scaling\n                    * 2\n                    / sampling_rate\n                )\n                y = self.exp_damping * (\n                    x_prev * self.sin_omega + y_prev * self.cos_omega\n                )\n\n                # pike generation and reset\n                spikes = (y &gt; threshold).astype(int)\n                y = (1 - spikes) * y + spikes * threshold  # reset y if spike occurred\n                x = (1 - spikes) * x  # reset x if spike occurred\n\n                return (x, y), (spikes, x, y)\n\n            initial_carry = (\n                jnp.zeros((self.num_neurons,)),\n                jnp.zeros((self.num_neurons,)),\n            )\n            _, (out_spikes, x, y) = jax.lax.scan(body_fun, initial_carry, audio)\n\n            time_ax = jnp.linspace(0, len(audio) / sampling_rate, len(audio))\n            event_v, address_v = jnp.where(out_spikes != 0)\n            event_time = time_ax[event_v]\n            event_address = address_v\n\n            return event_time, event_address, x, y\n\n        else:\n\n            @jax.jit\n            def body_fun(carry, input_val):\n                (x_prev, y_prev) = carry\n\n                # exact dynamics\n                x = (\n                    self.exp_damping\n                    * (x_prev * self.cos_omega - y_prev * self.sin_omega)\n                    + input_gain\n                    * input_val\n                    * self.damping_factors\n                    * damping_factor_scaling\n                    * 2\n                    / sampling_rate\n                )\n                y = self.exp_damping * (\n                    x_prev * self.sin_omega + y_prev * self.cos_omega\n                )\n\n                # pike generation and reset\n                spikes = (y &gt; threshold).astype(int)\n                y = (1 - spikes) * y + spikes * threshold  # reset y if spike occurred\n                x = (1 - spikes) * x  # reset x if spike occurred\n\n                return (x, y), spikes\n\n            initial_carry = (\n                jnp.zeros((self.num_neurons,)),\n                jnp.zeros((self.num_neurons,)),\n            )\n            _, out_spikes = jax.lax.scan(body_fun, initial_carry, audio)\n\n            time_ax = jnp.linspace(0, len(audio) / sampling_rate, len(audio))\n            event_v, address_v = jnp.where(out_spikes != 0)\n            event_time = time_ax[event_v]\n            event_address = address_v\n\n            return event_time, event_address\n\n    def dynamic_range_test(self):\n        max_input = []\n        min_input = []\n        dynamic_range = []\n\n        # search for all the central freqs of the array\n        for freq in self.frequencies:\n            # search for max input\n            test_time = 2 / freq\n            ampl_max = []\n            ampl_max.append(10)\n            time = jnp.arange(0, test_time, 1 / self.sampling_rate)\n            for search_step in jnp.arange(1, 20, 1):\n                event_time, event_address = self.__call__(\n                    audio=jnp.sin(jnp.pi * time * freq) * ampl_max[-1]\n                )\n                saturation = 0\n                for jj in jnp.arange(0, self.num_neurons, 1):\n                    count_x = jnp.count_nonzero(event_address == jj)\n                    if count_x &gt;= 1:  # ((test_time/freq)*0.99):\n                        saturation = saturation + 1\n                if saturation == 0:\n                    ampl_max.append(ampl_max[-1] + ampl_max[0] / 2**search_step)\n                elif saturation &gt;= 1:\n                    ampl_max.append(ampl_max[-1] - ampl_max[0] / 2**search_step)\n\n            # search for min input\n            ampl_min = []\n            ampl_min.append(10)\n            test_time = 50 / freq\n            time = jnp.arange(0, test_time, 1 / self.sampling_rate)\n            for search_step in jnp.arange(1, 20, 1):\n                event_time, event_address = self.__call__(\n                    audio=jnp.sin(jnp.pi * time * freq) * ampl_min[-1]\n                )\n\n                activation = 0\n                for jj in jnp.arange(0, self.num_neurons, 1):\n                    count_x = jnp.count_nonzero(event_address == jj)\n                    if count_x &gt; 0:\n                        activation = activation + 1\n                if activation == 0:\n                    ampl_min.append(ampl_min[-1] + ampl_min[0] / 2**search_step)\n                elif activation &gt;= 1:\n                    ampl_min.append(ampl_min[-1] - ampl_min[0] / 2**search_step)\n\n            max_input.append(ampl_max[-1])\n            min_input.append(ampl_min[-1])\n            dynamic_range.append(jnp.log10(ampl_max[-1] / ampl_min[-1]) * 2)\n\n        return dynamic_range, max_input, min_input\n</code></pre>"},{"location":"api/conversion_methods/#crusade.conversion_methods.standard_ADM","title":"<code>standard_ADM</code>","text":"<p>Standard Adaptive Delta Modulator (ADM) Implements a standard adaptive delta modulator that converts continuous audio signals into spike trains.</p> <p>Attributes:</p> Name Type Description <code>sampling_rate</code> <p>The sampling rate of the input audio signal.</p> <code>threshold</code> <code>float</code> <p>The threshold for spike generation.</p> <code>t_ref</code> <code>int</code> <p>The refractory period in number of samples. (Not implemented)</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Converts the input audio signal into spike trains.</p> Source code in <code>crusade/conversion_methods.py</code> <pre><code>class standard_ADM:\n    \"\"\"Standard Adaptive Delta Modulator (ADM)\n    Implements a standard adaptive delta modulator that converts continuous audio signals into spike trains.\n\n    Attributes:\n        sampling_rate: The sampling rate of the input audio signal.\n        threshold (float): The threshold for spike generation.\n        t_ref (int): The refractory period in number of samples. (Not implemented)\n\n    Methods:\n        __call__(audio, sampling_rate=None, threshold=None, t_ref=None): Converts the input audio signal into spike trains.\n    \"\"\"\n\n    # Based on Olympia Gallou code.\n\n    # TODO: Implement t_ref functionality\n    def __init__(\n        self, sampling_rate: float = 4410000, threshold: float = 0.01, t_ref: int = 1\n    ):\n        self.sampling_rate = sampling_rate\n        self.threshold = threshold\n        self.t_ref = t_ref\n\n    def __call__(\n        self,\n        audio: Float[Array, \"#time *N\"],\n        sampling_rate: Optional[float] = None,\n        threshold: Optional[float] = None,\n        t_ref: Optional[int] = None,\n        return_dense: bool = False,  # Use for vmap\n    ):\n        if sampling_rate is None:\n            sampling_rate = self.sampling_rate\n        if threshold is None:\n            threshold = self.threshold\n        if t_ref is None:\n            t_ref = self.t_ref\n\n        @jax.jit\n        def body_fun(carry, input_val):\n            (thr_DN, thr_UP, count_threshold) = carry\n\n            spikes = (\n                (input_val &gt;= thr_UP).astype(jnp.int32)\n                - (input_val &lt;= thr_DN).astype(jnp.int32)\n            ) * (count_threshold.astype(jnp.int32) &gt;= 1).astype(\n                jnp.int32\n            )  # Generate spikes based on thresholds and refractory period\n\n            count_threshold = (count_threshold + 1) * (\n                1 - jnp.abs(spikes)\n            )  # Reset counter if spike occurred\n            thr_UP = thr_UP * (1 - jnp.abs(spikes)) + (input_val + threshold) * jnp.abs(\n                spikes\n            )  # Update upper threshold if spike occurred\n            thr_DN = thr_DN * (1 - jnp.abs(spikes)) + (input_val - threshold) * jnp.abs(\n                spikes\n            )  # Update lower threshold if spike occurred\n\n            return (thr_DN, thr_UP, count_threshold), spikes\n\n        initial_carry = (\n            audio[0] - threshold,\n            audio[0] + threshold,\n            jnp.zeros_like(audio[0], dtype=jnp.int32),\n        )\n\n        _, out_spikes = jax.lax.scan(body_fun, initial_carry, audio)\n\n        time_ax = jnp.linspace(0, len(audio) / sampling_rate, len(audio))\n        spikes_2d = jnp.atleast_2d(out_spikes.T).T\n        N = spikes_2d.shape[1]\n\n        if return_dense:\n            max_spikes = spikes_2d.shape[0] * spikes_2d.shape[1]\n            time_idx, channel_idx = jnp.where(\n                spikes_2d != 0, size=max_spikes, fill_value=-1\n            )\n\n            safe_time_idx = jnp.where(time_idx &gt;= 0, time_idx, 0)\n            safe_channel_idx = jnp.where(channel_idx &gt;= 0, channel_idx, 0)\n\n            event_time = time_ax[safe_time_idx]\n            event_address = jnp.where(\n                spikes_2d[safe_time_idx, safe_channel_idx] == 1,\n                safe_channel_idx,\n                safe_channel_idx + N,\n            )\n\n            # Set invalid entries to inf to mark them as padding\n            event_time = jnp.where(time_idx &gt;= 0, event_time, jnp.inf)\n            event_address = jnp.where(time_idx &gt;= 0, event_address, jnp.inf)\n        else:\n            time_idx, channel_idx = jnp.where(spikes_2d != 0)\n            event_time = time_ax[time_idx]\n            event_address = jnp.where(\n                spikes_2d[time_idx, channel_idx] == 1, channel_idx, channel_idx + N\n            )\n\n        return event_time, event_address\n</code></pre>"},{"location":"api/conversion_methods/#crusade.conversion_methods.filterbank_ADM","title":"<code>filterbank_ADM</code>","text":"<p>Filterbank Adaptive Delta Modulator (ADM) Implements a filterbank-based adaptive delta modulator that converts continuous audio signals into spike trains.</p> <p>Attributes:</p> Name Type Description <code>sampling_rate</code> <p>The sampling rate of the input audio signal.</p> <code>num_neurons</code> <code>int</code> <p>The number of neurons in the filterbank.</p> <code>freq_min</code> <code>float</code> <p>The minimum frequency of the filterbank neurons.</p> <code>freq_max</code> <code>float</code> <p>The maximum frequency of the filterbank neurons.</p> <code>freq_distribution</code> <code>str</code> <p>The distribution of frequencies for the filterbank neurons ('linear', 'log', 'mel').</p> <code>delta</code> <code>float</code> <p>The delta value for threshold adaptation.</p> <code>t_ref</code> <code>float</code> <p>The refractory period in seconds.</p> <p>Methods:</p> Name Description <code>__call__</code> <p>Converts the input audio signal into spike trains.</p> Source code in <code>crusade/conversion_methods.py</code> <pre><code>class filterbank_ADM:\n    \"\"\"Filterbank Adaptive Delta Modulator (ADM)\n    Implements a filterbank-based adaptive delta modulator that converts continuous audio signals into spike trains.\n\n    Attributes:\n        sampling_rate: The sampling rate of the input audio signal.\n        num_neurons (int): The number of neurons in the filterbank.\n        freq_min (float): The minimum frequency of the filterbank neurons.\n        freq_max (float): The maximum frequency of the filterbank neurons.\n        freq_distribution (str): The distribution of frequencies for the filterbank neurons ('linear', 'log', 'mel').\n        delta (float): The delta value for threshold adaptation.\n        t_ref (float): The refractory period in seconds.\n\n    Methods:\n        __call__(audio, sampling_rate=None, threshold=None, t_ref=None): Converts the input audio signal into spike trains.\n    \"\"\"\n\n    # Based on Olympia Gallou code.\n\n    def __init__(\n        self,\n        sampling_rate: float = 4410000,\n        num_neurons: int = 32,\n        freq_min: float = 200,\n        freq_max: float = 20000,\n        freq_distribution: str = \"linear\",\n        delta: float = 0.02,\n        t_ref: float = 1e-3,\n    ):\n        self.sampling_rate = sampling_rate\n        self.num_neurons = num_neurons\n        self.freq_min = freq_min\n        self.freq_max = freq_max\n        self.freq_distribution = freq_distribution\n        self.delta = delta\n        self.t_ref = t_ref\n        self.step_ref = int(t_ref * sampling_rate)\n\n        self.frequencies, self.frequencies_bins = utils.frequency_bins_generator(\n            number_of_bins=self.num_neurons,\n            freq_min=self.freq_min,\n            freq_max=self.freq_max,\n            freq_distribution=self.freq_distribution,\n            bins_superimpose=0.0,\n        )\n\n        self.band_pass_window_array = []\n        self.low_pass_window_array = []\n        for i in range(self.num_neurons):\n            self.band_pass_window_array.append(\n                butter(\n                    N=2,\n                    Wn=[self.frequencies_bins[i], self.frequencies_bins[i + 1]],\n                    btype=\"bandpass\",\n                    fs=sampling_rate,\n                    output=\"sos\",\n                )\n            )\n            self.low_pass_window_array.append(\n                butter(N=2, Wn=100, btype=\"lowpass\", fs=sampling_rate, output=\"sos\")\n            )\n\n    def __call__(\n        self,\n        audio: Float[Array, \"#time\"],\n        sampling_rate: Optional[float] = None,\n        delta: Optional[float] = None,\n        t_ref: Optional[float] = None,\n    ):\n        if sampling_rate is None:\n            sampling_rate = self.sampling_rate\n        if delta is None:\n            delta = self.delta\n        if t_ref is None:\n            t_ref = self.t_ref\n\n        self.t_ref = t_ref\n        self.step_ref = int(t_ref * sampling_rate)\n\n        if sampling_rate != self.sampling_rate:\n            self.sampling_rate = sampling_rate\n            self.band_pass_window_array = []\n            self.low_pass_window_array = []\n            for i in range(self.num_neurons):\n                self.band_pass_window_array.append(\n                    butter(\n                        N=2,\n                        Wn=[self.frequencies_bins[i], self.frequencies_bins[i + 1]],\n                        btype=\"bandpass\",\n                        fs=sampling_rate,\n                        output=\"sos\",\n                    )\n                )\n                self.low_pass_window_array.append(\n                    butter(N=2, Wn=100, btype=\"lowpass\", fs=sampling_rate, output=\"sos\")\n                )\n\n        audio_bands = utils.bandpass_signal(\n            num_bands=self.num_neurons,\n            audio=audio,\n            band_pass_window_array=self.band_pass_window_array,\n            low_pass_window_array=self.low_pass_window_array,\n        )\n\n        # for audio_band in audio_bands:\n        #     plt.plot(audio_band[::1000])\n        # plt.legend()\n        # plt.show()\n\n        @jax.jit\n        def body_fun(carry, input_val):\n            (thr_DN, thr_UP, count_threshold) = carry\n\n            spikes = (\n                (input_val &gt;= thr_UP).astype(int) - (input_val &lt;= thr_DN).astype(int)\n            ) * (count_threshold.astype(int) &gt;= self.step_ref).astype(\n                int\n            )  # Generate spikes based on thresholds and refractory period (in steps)\n\n            count_threshold = (count_threshold + 1) * (\n                1 - jnp.abs(spikes)\n            )  # Reset counter if spike occurred\n            thr_UP = thr_UP * (1 - jnp.abs(spikes)) + (input_val + delta) * jnp.abs(\n                spikes\n            )  # Update upper threshold if spike occurred\n            thr_DN = thr_DN * (1 - jnp.abs(spikes)) + (input_val - delta) * jnp.abs(\n                spikes\n            )  # Update lower threshold if spike occurred\n\n            return (thr_DN, thr_UP, count_threshold), spikes\n\n        initial_carry = (\n            audio_bands[:, 0] - delta,\n            audio_bands[:, 0] + delta,\n            jnp.zeros((self.num_neurons,)),\n        )\n        _, out_spikes = jax.lax.scan(body_fun, initial_carry, audio_bands.T)\n\n        time_ax = jnp.linspace(0, len(audio) / sampling_rate, len(audio))\n        event_v, address_v = jnp.where(out_spikes != 0)\n\n        event_time = time_ax[event_v]\n        event_address = address_v\n        event_magnitude = out_spikes[event_v, address_v]\n\n        return event_time, event_address, event_magnitude\n\n    def dynamic_range_test(self):\n        max_input = []\n        min_input = []\n        dynamic_range = []\n\n        test_time = 0.1\n        time = jnp.arange(0, test_time, 1 / self.sampling_rate)\n        for freq in self.frequencies:\n            ampl_max = []\n            ampl_max.append(10)\n            for search_step in jnp.arange(1, 20, 1):\n                _, event_address, _ = self.__call__(\n                    audio=jnp.sin(jnp.pi * time * freq)\n                    * (\n                        jnp.abs(\n                            2 * (time / test_time - jnp.floor(time / test_time + 0.5))\n                        )\n                    )\n                    * ampl_max[-1]\n                )\n\n                saturation = 0\n                for jj in jnp.arange(0, self.num_neurons, 1):\n                    count_x = jnp.count_nonzero(event_address == jj)\n                    # print(count_x)\n                    if count_x &gt;= ((test_time / self.t_ref) * 0.95):\n                        saturation = saturation + 1\n                if saturation == 0:\n                    ampl_max.append(ampl_max[-1] + ampl_max[0] / 2**search_step)\n                elif saturation &gt;= 1:\n                    ampl_max.append(ampl_max[-1] - ampl_max[0] / 2**search_step)\n\n            ampl_min = []\n            ampl_min.append(10)\n            for search_step in jnp.arange(1, 20, 1):\n                _, event_address, _ = self.__call__(\n                    audio=jnp.sin(jnp.pi * time * freq)\n                    * (\n                        jnp.abs(\n                            2 * (time / test_time - jnp.floor(time / test_time + 0.5))\n                        )\n                    )\n                    * ampl_min[-1]\n                )\n\n                activation = 0\n                for jj in jnp.arange(0, self.num_neurons, 1):\n                    count_x = jnp.count_nonzero(event_address == jj)\n                    if count_x &gt; 0:\n                        activation = activation + 1\n                if activation == 0:\n                    ampl_min.append(ampl_min[-1] + ampl_min[0] / 2**search_step)\n                elif activation &gt;= 1:\n                    ampl_min.append(ampl_min[-1] - ampl_min[0] / 2**search_step)\n\n            dynamic_range.append(2 * jnp.log10(ampl_max[-1] / ampl_min[-1]))\n            max_input.append(ampl_max[-1])\n            min_input.append(ampl_min[-1])\n\n        return dynamic_range, max_input, min_input\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#crusade.utils","title":"<code>crusade.utils</code>","text":""},{"location":"api/utils/#crusade.utils-functions","title":"Functions","text":""},{"location":"api/utils/#crusade.utils.download_dataset","title":"<code>download_dataset(name='rfcx/frugalai', streaming=False, token=None, split='train')</code>","text":"<p>Downloads the RFCx FrugalAI dataset using the Hugging Face datasets library. to login:</p> <pre><code>$ huggingface-cli login\n</code></pre> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <p>The downloaded dataset containing training, validation, and test splits.</p> Source code in <code>crusade/utils.py</code> <pre><code>def download_dataset(name=\"rfcx/frugalai\", streaming=False, token=None, split=\"train\"):\n    \"\"\"Downloads the RFCx FrugalAI dataset using the Hugging Face datasets library.\n    to login:\n\n    ```bash\n    $ huggingface-cli login\n    ```\n\n    Returns:\n        DatasetDict: The downloaded dataset containing training, validation, and test splits.\n    \"\"\"\n    if token is not None:\n        dataset = load_dataset(name, streaming=streaming, token=token, split=split)\n    else:\n        dataset = load_dataset(name, streaming=streaming, split=split)\n\n    return dataset\n</code></pre>"},{"location":"api/utils/#crusade.utils.low_pass_filter","title":"<code>low_pass_filter(modulated_signal, window_size=301, gain=4.65415)</code>","text":"<p>Applies a low-pass filter to the modulated signal using a Hann window.</p> <p>Parameters:</p> Name Type Description Default <code>modulated_signal</code> <code>float</code> <p>Input modulated signal (e.g., spike train).</p> required <code>window_size</code> <code>int</code> <p>Size of the Hann window for filtering.</p> <code>301</code> <code>gain</code> <code>float</code> <p>Gain factor to scale the filtered signal.</p> <code>4.65415</code> <p>Returns:</p> Name Type Description <code>float</code> <p>The filtered signal.</p> Source code in <code>crusade/utils.py</code> <pre><code>def low_pass_filter(modulated_signal, window_size=301, gain=4.65415):\n    \"\"\"Applies a low-pass filter to the modulated signal using a Hann window.\n\n    Args:\n        modulated_signal (float): Input modulated signal (e.g., spike train).\n        window_size (int): Size of the Hann window for filtering.\n        gain (float): Gain factor to scale the filtered signal.\n\n    Returns:\n        float: The filtered signal.\n    \"\"\"\n    kernel = windows.flattop(window_size) / window_size  # Create a Hann window kernel\n    filtered_signal = jax.scipy.signal.convolve(\n        modulated_signal * gain, kernel, mode=\"same\"\n    )\n\n    return filtered_signal\n</code></pre>"},{"location":"api/utils/#crusade.utils.audio_resampling_and_scaling","title":"<code>audio_resampling_and_scaling(audio: Float[Array, '#time'], original_frequency: float, target_frequency: float, scaling_factor=1.0) -&gt; Float[Array, '#time']</code>","text":"<p>Resamples and scales the input audio signal.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>Array</code> <p>Input audio signal.</p> required <code>original_frequency</code> <code>float</code> <p>Original sampling frequency of the audio signal.</p> required <code>target_frequency</code> <code>float</code> <p>Target sampling frequency for resampling.</p> required <code>scaling_factor</code> <code>float or str</code> <p>Scaling factor or method ('normalize') for scaling the audio signal.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>Array</code> <code>Float[Array, '#time']</code> <p>Resampled and scaled audio signal.</p> Source code in <code>crusade/utils.py</code> <pre><code>def audio_resampling_and_scaling(\n    audio: Float[Array, \"#time\"],\n    original_frequency: float,\n    target_frequency: float,\n    scaling_factor=1.0,\n) -&gt; Float[Array, \"#time\"]:\n    \"\"\"Resamples and scales the input audio signal.\n\n    Args:\n        audio (Array): Input audio signal.\n        original_frequency (float): Original sampling frequency of the audio signal.\n        target_frequency (float): Target sampling frequency for resampling.\n        scaling_factor (float or str): Scaling factor or method ('normalize') for scaling the audio signal.\n\n    Returns:\n        Array: Resampled and scaled audio signal.\n    \"\"\"\n    if isinstance(scaling_factor, float):\n        if scaling_factor != 1.0:\n            audio = audio * scaling_factor\n    elif isinstance(scaling_factor, str):\n        if scaling_factor == \"normalize\":\n            audio = audio / jnp.max(jnp.abs(audio))\n\n        elif scaling_factor == \"mulaw\":\n            audio = audio / jnp.max(jnp.abs(audio))\n            audio = mu_encoding(audio, mu=255)\n\n    if original_frequency != target_frequency:\n        number_of_points = len(audio)\n        audio = jnp.asarray(\n            resample(\n                audio, int(number_of_points * (target_frequency // original_frequency))\n            )\n        )\n\n    return audio\n</code></pre>"},{"location":"api/utils/#crusade.utils.mu_encoding","title":"<code>mu_encoding(signal, mu=255)</code>","text":"<p>Applies mu-law encoding to the input signal (it should be pronounced mi).</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>float</code> <p>Input audio signal.</p> required <code>mu</code> <code>int</code> <p>Mu parameter for mu-law encoding.</p> <code>255</code> <p>Returns:</p> Name Type Description <code>float</code> <p>Mu-law encoded signal.</p> Source code in <code>crusade/utils.py</code> <pre><code>def mu_encoding(signal, mu=255):\n    \"\"\"Applies mu-law encoding to the input signal (it should be pronounced mi).\n\n    Args:\n        signal (float): Input audio signal.\n        mu (int): Mu parameter for mu-law encoding.\n\n    Returns:\n        float: Mu-law encoded signal.\n    \"\"\"\n    signal = jnp.clip(signal, -1.0, 1.0)\n    encoded_signal = jnp.sign(signal) * jnp.log1p(mu * jnp.abs(signal)) / jnp.log1p(mu)\n    return encoded_signal\n</code></pre>"}]}